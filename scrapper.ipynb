{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "141f2d9d",
   "metadata": {},
   "source": [
    "# LinkedIn job search (guest endpoint)\n",
    "\n",
    "This notebook uses LinkedIn’s public “guest” search endpoint to collect job listings. Automated access to LinkedIn may be restricted by their Terms of Service—use responsibly, keep request rates low, and prefer official APIs or approved data sources whenever possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75573ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search parameters\n",
    "job_titles = [\"data scientist\", \"machine learning engineer\"]\n",
    "skills = [\"python\", \"git\"]\n",
    "industry = [\"energy\", \"oil & gas\"]\n",
    "\n",
    "# Options: \"past 24 hours\", \"past week\", \"past month\", \"any time\"\n",
    "job_post_date = \"past 24 hours\"\n",
    "\n",
    "# Optional filters\n",
    "location = \"United States\"  # change to your preferred location\n",
    "max_results_per_title = 50   # cap per job title\n",
    "pause_seconds = 0.6          # polite delay between requests\n",
    "include_description = False  # set True to fetch job descriptions (slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5a9fc89",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrequests\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m     14\u001b[39m HEADERS = {\n\u001b[32m     15\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m\"\u001b[39m: (\n\u001b[32m     16\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     19\u001b[39m     )\n\u001b[32m     20\u001b[39m }\n\u001b[32m     22\u001b[39m TIME_FILTERS = {\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpast 24 hours\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mr86400\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpast week\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mr604800\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     25\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mpast month\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mr2592000\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     26\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33many time\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m }\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# If needed, install deps: pip install requests beautifulsoup4 pandas\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import time\n",
    "from typing import Iterable, List, Dict\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "        \"Chrome/120.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "TIME_FILTERS = {\n",
    "    \"past 24 hours\": \"r86400\",\n",
    "    \"past week\": \"r604800\",\n",
    "    \"past month\": \"r2592000\",\n",
    "    \"any time\": \"\",\n",
    "}\n",
    "\n",
    "\n",
    "def normalize(text: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", text or \"\").strip().lower()\n",
    "\n",
    "\n",
    "def build_query(job_title: str, skills: List[str], industries: List[str]) -> str:\n",
    "    parts = [job_title] + (skills or []) + (industries or [])\n",
    "    return \" \".join(p for p in parts if p).strip()\n",
    "\n",
    "\n",
    "def build_url(query: str, location: str, start: int, time_filter: str) -> str:\n",
    "    params = {\n",
    "        \"keywords\": query,\n",
    "        \"location\": location,\n",
    "        \"start\": start,\n",
    "    }\n",
    "    tpr = TIME_FILTERS.get(time_filter.lower(), \"\")\n",
    "    if tpr:\n",
    "        params[\"f_TPR\"] = tpr\n",
    "    return \"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?\" + urlencode(params)\n",
    "\n",
    "\n",
    "def fetch_jobs_page(url: str) -> str:\n",
    "    resp = requests.get(url, headers=HEADERS, timeout=20)\n",
    "    resp.raise_for_status()\n",
    "    return resp.text\n",
    "\n",
    "\n",
    "def parse_jobs(html: str) -> List[Dict[str, str]]:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    cards = soup.select(\"li\")\n",
    "    results: List[Dict[str, str]] = []\n",
    "\n",
    "    for card in cards:\n",
    "        title_el = card.select_one(\"h3\")\n",
    "        company_el = card.select_one(\"h4\")\n",
    "        location_el = card.select_one(\".job-search-card__location\")\n",
    "        time_el = card.select_one(\"time\")\n",
    "        link_el = card.select_one(\"a\")\n",
    "        urn = card.get(\"data-entity-urn\", \"\")\n",
    "\n",
    "        title = title_el.get_text(strip=True) if title_el else \"\"\n",
    "        company = company_el.get_text(strip=True) if company_el else \"\"\n",
    "        location = location_el.get_text(strip=True) if location_el else \"\"\n",
    "        date = time_el.get_text(strip=True) if time_el else \"\"\n",
    "        link = link_el.get(\"href\", \"\") if link_el else \"\"\n",
    "        job_id = urn.split(\":\")[-1] if urn else \"\"\n",
    "\n",
    "        if not title and not company:\n",
    "            continue\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                \"job_id\": job_id,\n",
    "                \"title\": title,\n",
    "                \"company\": company,\n",
    "                \"location\": location,\n",
    "                \"date_posted\": date,\n",
    "                \"link\": link,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def fetch_description(job_url: str) -> str:\n",
    "    if not job_url:\n",
    "        return \"\"\n",
    "    resp = requests.get(job_url, headers=HEADERS, timeout=20)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    desc_el = soup.select_one(\".description__text, .show-more-less-html__markup\")\n",
    "    return desc_el.get_text(\" \", strip=True) if desc_el else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a012d97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_linkedin_jobs(\n",
    "    job_titles: List[str],\n",
    "    skills: List[str],\n",
    "    industries: List[str],\n",
    "    job_post_date: str,\n",
    "    location: str,\n",
    "    max_results_per_title: int = 50,\n",
    "    pause_seconds: float = 0.6,\n",
    "    include_description: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    all_results: List[Dict[str, str]] = []\n",
    "\n",
    "    for job_title in job_titles:\n",
    "        query = build_query(job_title, skills, industries)\n",
    "        collected = 0\n",
    "        start = 0\n",
    "\n",
    "        while collected < max_results_per_title:\n",
    "            url = build_url(query=query, location=location, start=start, time_filter=job_post_date)\n",
    "            html = fetch_jobs_page(url)\n",
    "            batch = parse_jobs(html)\n",
    "\n",
    "            if not batch:\n",
    "                break\n",
    "\n",
    "            if include_description:\n",
    "                for row in batch:\n",
    "                    row[\"description\"] = fetch_description(row.get(\"link\", \"\"))\n",
    "\n",
    "            all_results.extend(batch)\n",
    "            collected += len(batch)\n",
    "            start += len(batch)\n",
    "\n",
    "            time.sleep(pause_seconds)\n",
    "\n",
    "    df = pd.DataFrame(all_results)\n",
    "\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # Keyword-based filtering\n",
    "    def match_any(haystack: str, needles: Iterable[str]) -> bool:\n",
    "        needles = [n for n in (needles or []) if n]\n",
    "        if not needles:\n",
    "            return True\n",
    "        return any(n in haystack for n in needles)\n",
    "\n",
    "    text_cols = [\"title\", \"company\", \"location\", \"description\"]\n",
    "    df[\"_haystack\"] = (\n",
    "        df[text_cols]\n",
    "        .fillna(\"\")\n",
    "        .agg(\" \".join, axis=1)\n",
    "        .map(normalize)\n",
    "    )\n",
    "\n",
    "    skill_terms = [normalize(s) for s in skills]\n",
    "    industry_terms = [normalize(i) for i in industries]\n",
    "\n",
    "    df = df[df[\"_haystack\"].map(lambda t: match_any(t, skill_terms))]\n",
    "    df = df[df[\"_haystack\"].map(lambda t: match_any(t, industry_terms))]\n",
    "\n",
    "    df = df.drop(columns=[\"_haystack\"]).drop_duplicates(subset=[\"job_id\", \"link\", \"title\", \"company\"])\n",
    "    return df\n",
    "\n",
    "\n",
    "results = search_linkedin_jobs(\n",
    "    job_titles=job_titles,\n",
    "    skills=skills,\n",
    "    industries=industry,\n",
    "    job_post_date=job_post_date,\n",
    "    location=location,\n",
    "    max_results_per_title=max_results_per_title,\n",
    "    pause_seconds=pause_seconds,\n",
    "    include_description=include_description,\n",
    ")\n",
    "\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_path = \"linkedin_jobs.csv\"\n",
    "results.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Saved {len(results)} rows to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
